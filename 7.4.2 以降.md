# 7.4.2 続き

GNNは
- 各ノードの保持している情報を初期値とする状態を持っている
- 各ノードの情報は自ノードと接続されているノードの値を集約して更新される
	- 数式(7.19)
- 計算はグラフのすべてのノードについて独立に並行して行っている
	- 図7.4
- 各ノードの状態の更新は多層ネットワークとして表現できる
	- 図7.5
	- $L$回の更新によって$L$個のエッジをたどっていった分のノードの情報をとらえる
- 最終的に得られる値をノードのembedding(埋め込み)という

GNNがCNNやTransformerと違う点は
- (7.19)の計算、層への伝播の計算においてグラフの構造を反映している点
	- ノード$i$に対応するユニットが受け取る入力は、
		- 要素数が$\#\mathcal{N}_{i}$の集合となる
- Transformerは全入力の要素間で相互作用が発生する
	- GNNは、ノードが間でのみ相互作用が発生する

数式(7.19)が含む$\mathbf{f}^{(l)}$は学習するべき重みを持つネットワークであり、
これを設計する上でほしい特徴をいかに整理する。

- 更新の際にノード$i$の隣接ノード$\mathcal{N}_{i}$の要素同士を区別しない
	- 状態の違いは区別するが、処理は同じ
- 自ノードの状態と集約・結合する処理を加える
	- これらの処理は分割して行われる
	- 各GNNの特徴はこの2つの処理による

代表的なGNNであるGCNは以下の数式(7.20)で表現できる

$$
\mathbf{z}_i^{(l+1)}=\operatorname{ReLU}\left(\sum_{j \in \mathcal{N}_i \cup\{i\}} \frac{1}{\sqrt{\operatorname{deg}(i) \cdot \operatorname{deg}(j)}} \mathbf{W}^{(l+1)} \mathbf{z}_j^{(l)}\right)
$$

これを1次元CNNと比較する(図7.6)と、以下のようになる

- ノード3は1以外のすべてとつながっている(図7.6(a))
	- ノード3について、$\text{deg}(3)$は3となる(図7.6(a)から)
		- deg(x)はノードxがいくつのエッジとつながっているかを指す
- GCNにおいて、ノード3はノード2から5の影響を受ける(図7.6(b)から)
	- ノード3自身には重み行列$\mathbf{W}_{0}$を、それ以外に対しては$\mathbf{W}_{1}$をかけている(図7.6(b))
	- この点で自ノードか隣接ノードであるかの区別しかしていない
		- 反面、層間のつながりはノードごとに異なっている様子を表現している
- 図の1次元CNNは、サイズが3のフィルタを1次元畳み込みしたもの(図7.6(c))
	- ノード$i$は端っこを除けば$i-1,i,i+1$のノードから影響を受ける
	- すべてのノードに対してかける重みがそれぞれ異なる
		- GCNでは自ノード以外の重みは同じだった
		- 影響するすべてのノードに対して別の処理をしているといえる
			- CNNはどのノードも関係するノードが同じであるとしている
			- ノードごとのエッジによる関係構造を区別していない

# 7.4.3 代表的なGNN

ここでは以下のネットワークを紹介する。
- GCN、グラフ畳み込みネットワーク
- GAT、グラフ注意ネットワーク
- GIN、グラフ同形ネットワーク

GCNについて
- 以下の更新式でネットワークを更新する
- 次元の平方根で除算するのが特徴
- 重みが同じ層の全ノードで共通
	- 自ノードについては別の重みを使うことができる

$$
\mathbf{z}_i^{(l+1)}=\operatorname{ReLU}\left(\sum_{j \in \mathcal{N}_i \cup\{i\}} \frac{1}{\sqrt{\operatorname{deg}(i) \cdot \operatorname{deg}(j)}} \mathbf{W}^{(l+1)} \mathbf{z}_j^{(l)}\right)
$$

GATについて

- 以下の更新式でネットワークを更新する
- transformer同様マルチヘッド注意を利用することができる
- $\mathbf{a}$は学習対象となるパラメータ

$$
\mathbf{h}_j^{(l)}=\mathbf{W}^{(l+1)} \mathbf{z}_j^{(l)}
$$

$$
a_{i j}=\operatorname{softmax}\left[a\left(\mathbf{h}_i^{(l)}, \mathbf{h}_j^{(l)}\right)\right]
$$

$$
\mathbf{z}_i^{(l+1)}=\operatorname{ReLU}\left(\sum_{j \in \mathcal{N}_i \cup\{i\}} a_{i j} \mathbf{h}_j^{(l)}\right)
$$

上記2点は、
- グラフ上の平滑化計算でしかないと指摘されている
- 層の数、つまり更新回数が増加すれば性能が低下する傾向がある
- GINはそれを改善するために提案された

GINについて

- MLPは複数の全結合層を重ねたもので、重みは学習対象
- $\epsilon$は学習によって決定する

$$
\mathbf{z}_i^{(l+1)}=\operatorname{MLP}^{(l+1)}\left(\left(1+\epsilon^{(l+1)}\right) \mathbf{z}_i^{(l)}+\sum_{j \in \mathcal{N}_i} \mathbf{z}_j^{(l)}\right)
$$

# 7.4.4 グラフを用いて行う推論

ほとんどの場合、GNNを用いてノードの埋め込み、embeddingsを得てそれを活用する。

ここでは、以下の応用を紹介する。

- ノードのクラス分類
- ノードのクラスタリング(コミュニティ検出)
- グラフのクラス分類
- 接続予測
- 2部グラフの接続予測
- マルチグラフに対する推論

## ノードのクラス分類

各ノードを定められたクラスに分類する問題
- 半教師学習として定式化されることが多い
	- 一部のノードのみ正解クラスラベルが付与されている
	- そうでないノードのクラスラベルを推測する
- ベンチマークテストは、論文の属性と引用関係から学術分野を予測させるものがある
	- 引用は向きがあるが、それを無視する場合もある
- 推論は、ノードの埋め込みを元に全結合層+ソフトマックス関数を適用するなどで行われる
	- 交差エントロピーの最小化をする
- 応用の問題に関連するグラフは図7.7

## ノードのクラスタリング(コミュニティ検出)

埋め込みからノードをクラスタリングする問題

## グラフのクラス分類

与えられたグラフ自体のクラスタリングを行う
- 例えば数式(7.21)のような集約結果を使って推論する
	- これだけだとグラフの構造が反映されない
	- 改善方法は多数検討されている
- タンパク質の構造を元に酵素としての働きを分類するタスクなどがある
	- データセットPROTEINS、ENZYMESなどが知られている

## 接続予測

グラフの指定されたノード間にエッジがあるかを予測する問題
- ノード間のエッジの情報が部分的に欠損している場合に推測する
- 時間とともに変化するグラフを対象に考えることも
- レコメンデーションや、知識ベースの構築に用いられる
- 与えられた埋め込みを使ってグラフオートエンコーダを用いる方法がある
	- 低次元情報で元のグラフを表現する手法
- 数式(7.25)などの、ノードの類似度からエッジの有無を推測するような計算を行う

## 2部グラフの接続予測

ユーザと商品など、個別のノードで表現して購買関係をエッジで表現するなどしたグラフを2部グラフとしてもっておいて、その接続予測をするのがこの問題である。
- 商品同士、ユーザ同士はエッジをもたない構造が2部グラフ(図7.7(b))
- このような推論は推薦に利用できる
- エッジを通じてユーザによる各商品のレーティングが付与される場合などがある


## マルチグラフに対する推論

ノード間に複数のエッジがあるようなグラフをマルチグラフと呼ぶ
- 知識ベースを構成するグラフはマルチグラフになる
- 図7.7(c)などの場合がそう
- 関係には種類があり、エッジにも無機や種類がある

# 7.2.4 畳み込み層のための注意機構

CNNでも注意機構が使われている
- 代表的手法としてSE-Net(圧縮励起ネットワーク)がある
	- ResNetの改良が注意機構を利用することで可能